{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T10:17:51.403149Z",
     "iopub.status.busy": "2024-12-19T10:17:51.402817Z",
     "iopub.status.idle": "2024-12-19T10:17:53.283267Z",
     "shell.execute_reply": "2024-12-19T10:17:53.282780Z",
     "shell.execute_reply.started": "2024-12-19T10:17:51.403126Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T10:17:54.648506Z",
     "iopub.status.busy": "2024-12-19T10:17:54.648061Z",
     "iopub.status.idle": "2024-12-19T10:17:59.718676Z",
     "shell.execute_reply": "2024-12-19T10:17:59.718171Z",
     "shell.execute_reply.started": "2024-12-19T10:17:54.648484Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 18:17:55.806709: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-19 18:17:56.077294: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-19 18:17:56.949477: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/local/lib/python3.10/site-packages/torch/__init__.py:690: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "德语分词结果: ['Das', 'ist', 'ein', 'Beispieltext', '.']\n",
      "英语分词结果: ['This', 'is', 'an', 'example', 'text', '.']\n"
     ]
    }
   ],
   "source": [
    "def my_tokenizer():\n",
    "    tokenizer = {}\n",
    "    # 这里我们尝试用spacy来定义分词器\n",
    "    tokenizer['de'] = get_tokenizer('spacy', language='de_core_news_sm')    # 德语\n",
    "    tokenizer['en'] = get_tokenizer('spacy', language='en_core_web_sm')     # 英语\n",
    "    return tokenizer\n",
    "\n",
    "# 加载分词器\n",
    "tokenizers = my_tokenizer()\n",
    "\n",
    "# 使用分词器分词\n",
    "de_sentence = \"Das ist ein Beispieltext.\"\n",
    "en_sentence = \"This is an example text.\"\n",
    "\n",
    "# 调用分词器\n",
    "de_tokens = tokenizers['de'](de_sentence)\n",
    "en_tokens = tokenizers['en'](en_sentence)\n",
    "\n",
    "print(\"德语分词结果:\", de_tokens)\n",
    "print(\"英语分词结果:\", en_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-12-19T10:18:56.357369Z",
     "iopub.status.busy": "2024-12-19T10:18:56.357026Z",
     "iopub.status.idle": "2024-12-19T10:18:59.568451Z",
     "shell.execute_reply": "2024-12-19T10:18:59.567931Z",
     "shell.execute_reply.started": "2024-12-19T10:18:56.357348Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词 '<unk>' 对应的索引: 0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(tokenizer, filepath, min_freq=1, specials=None):\n",
    "    \"\"\"\n",
    "    vocab = Vocab(counter, specials=specials)\n",
    "    \n",
    "    print(vocab.itos)   # 得到一个列表，返回此表中的每一个词\n",
    "    # ['<unk>', '<pad>', '<bos>', '<eos>', '.', 'a', 'are', 'A', 'Two', 'in', 'men',...]\n",
    "    print(vocab.itos[2])  # 通过索引返回得到词表中对应的词；\n",
    "    \n",
    "    print(vocab.stoi)  # 得到一个字典，返回词表中每个词的索引；\n",
    "    # {'<unk>': 0, '<pad>': 1, '<bos>': 2, '<eos>': 3, '.': 4, 'a': 5, 'are': 6,...}\n",
    "    print(vocab.stoi['are'])  # 通过单词返回得到词表中对应的索引\n",
    "    \"\"\"\n",
    "    if specials is None:\n",
    "        specials = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "    counter = Counter()\n",
    "    with open(filepath, encoding='utf8') as f:\n",
    "        for string_ in f:\n",
    "            counter.update(tokenizer(string_))\n",
    "    return vocab(counter, specials=specials, min_freq=min_freq)\n",
    "\n",
    "tokenizer = my_tokenizer()\n",
    "filepath = './data/train.de'\n",
    "de_vocab = build_vocab(tokenizer['de'], filepath)\n",
    "\n",
    "# print(de_vocab.get_itos())  # 查看词典\n",
    "# print(de_vocab.get_stoi())  # 查看string-to-index字典\n",
    "index = de_vocab.get_stoi()['<unk>']  \n",
    "print(f\"词 '<unk>' 对应的索引: {index}\")\n",
    "print(de_vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T10:19:01.267312Z",
     "iopub.status.busy": "2024-12-19T10:19:01.266952Z",
     "iopub.status.idle": "2024-12-19T10:19:01.271920Z",
     "shell.execute_reply": "2024-12-19T10:19:01.271347Z",
     "shell.execute_reply.started": "2024-12-19T10:19:01.267279Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Zwei', 'junge', 'weiße', 'Männer', 'sind', 'im', 'Freien', 'in', 'der', 'Nähe', 'vieler', 'Büsche', '.']\n"
     ]
    }
   ],
   "source": [
    "de_iter = iter(open(filepath, encoding='utf8'))\n",
    "for line in de_iter:\n",
    "    line = line.rstrip(\"\\n\")\n",
    "    tokens = tokenizer['de'](line)\n",
    "    print(tokens)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T10:19:03.088741Z",
     "iopub.status.busy": "2024-12-19T10:19:03.088429Z",
     "iopub.status.idle": "2024-12-19T10:19:03.100523Z",
     "shell.execute_reply": "2024-12-19T10:19:03.099957Z",
     "shell.execute_reply.started": "2024-12-19T10:19:03.088719Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LoadEnglishGermanDataset():\n",
    "    def __init__(self, train_file_paths=None, tokenizer=None,\n",
    "                 batch_size=2, min_freq=1):\n",
    "        # 根据训练语料建立英语和德语各自的字典\n",
    "        self.tokenizer = tokenizer()\n",
    "        self.de_vocab = build_vocab(self.tokenizer['de'], filepath=train_file_paths[0], min_freq=min_freq)\n",
    "        self.en_vocab = build_vocab(self.tokenizer['en'], filepath=train_file_paths[1], min_freq=min_freq)\n",
    "        self.specials = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "        self.PAD_IDX = self.de_vocab['<pad>']\n",
    "        self.BOS_IDX = self.de_vocab['<bos>']\n",
    "        self.EOS_IDX = self.de_vocab['<EOS>']\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def data_process(self, filepaths):\n",
    "        \"\"\"\n",
    "        将每一句话中的每一个词根据词典转换成索引的形式\n",
    "        param filepaths:\n",
    "        return: \n",
    "        \"\"\"\n",
    "        raw_de_iter = iter(open(filepaths[0], encoding='utf8'))\n",
    "        raw_en_iter = iter(open(filepaths[1], encoding='utf8'))\n",
    "        data = []\n",
    "        logging.info(f\"### 正在将数据集 {filepaths} 转换成 Token ID\")\n",
    "        for (raw_de, raw_en) in tqdm(zip(raw_de_iter, raw_en_iter), ncols=80):\n",
    "            de_tensor_ = torch.tensor([self.de_vocab[token] if token in self.de_vocab else self.de_vocab['<unk>'] for token in\n",
    "                                       self.tokenizer['de'](raw_de.rstrip(\"\\n\"))], dtype=torch.long) \n",
    "            en_tensor_ = torch.tensor([self.en_vocab[token] if token in self.en_vocab else self.en_vocab['<unk>'] for token in\n",
    "                                       self.tokenizer['en'](raw_en.rstrip(\"\\n\"))], dtype=torch.long)\n",
    "            data.append((de_tensor_, en_tensor_))\n",
    "        return data\n",
    "    \n",
    "    def load_train_val_test_data(self, train_file_paths, val_file_paths, test_file_paths):\n",
    "        train_data = self.data_process(train_file_paths)\n",
    "        val_data = self.data_process(val_file_paths)\n",
    "        test_data = self.data_process(test_file_paths)\n",
    "        train_iter = DataLoader(train_data, batch_size=self.batch_size,\n",
    "                                shuffle=True, collate_fn=self.generate_batch)\n",
    "        valid_iter = DataLoader(val_data, batch_size=self.batch_size,\n",
    "                                shuffle=False, collate_fn=self.generate_batch)\n",
    "        test_iter = DataLoader(test_data, batch_size=self.batch_size,\n",
    "                                shuffle=False, collate_fn=self.generate_batch)\n",
    "    \n",
    "    def generate_batch(self, data_batch):\n",
    "        \"\"\"\n",
    "        自定义一个函数来对每个batch的样本进行处理，该函数将作为一个参数传入到类DataLoader中。\n",
    "        由于在DataLoader中是对每一个batch的数据进行处理，所以这就意味着下面的pad_sequence操作，最终表现出来的结果就是\n",
    "        不同的样本，padding后在同一个batch中长度是一样的，而在不同的batch之间可能是不一样的。因为pad_sequence是以一个\n",
    "        batch的样本为标准对其它样本进行padding\n",
    "        param data_batch:\n",
    "        return:\n",
    "        \"\"\"\n",
    "        de_batch, en_batch = [], []\n",
    "        for (de_item, en_item) in data_batch:   # 开始对一个batch中的每一个样本进行处理\n",
    "            de_batch.append(de_item)    # 编码器输入序列不需要加起止符\n",
    "            # 解码器部分，在每个idx序列的首尾加上 起始token 和 结束token\n",
    "            en = torch.cat([torch.tensor([self.BOS_IDX]), en_item, torch.tensor([self.EOS_IDX])], dim=0)\n",
    "            en_batch.append(en)\n",
    "        # 以最长的序列为标准进行填充\n",
    "        de_batch = pad_sequence(de_batch, padding_value=self.PAD_IDX)   # [de_len, batch_size]\n",
    "        en_batch = pad_sequence(en_batch, padding_value=self.PAD_IDX)   # [en_len, batch_size]\n",
    "        return de_batch, en_batch\n",
    "\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz, device):\n",
    "        # 用于生成decoder阶段的attention_mask方阵\n",
    "        mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "    \n",
    "    def create_mask(self, src, tgt, device='cpu'):\n",
    "        src_seq_len = src.shape[0]\n",
    "        tgt_sen_len = tgt.shape[0]\n",
    "        \n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt_sen_len, device)    # [tgt_len, tgt_len]\n",
    "        # Decoder的注意力Mask输入，用于掩盖当前position之后的position，所以这里是一个对称矩阵\n",
    "        \n",
    "        src_mask = torch.zeros((src_seq_len, src_seq_len), device=device).type(torch.bool)\n",
    "        # Encoder的注意力Mask输入，这部分其实对于Encoder来说是没有用的，所以这里全是0\n",
    "        \n",
    "        src_padding_mask = (src == self.PAD_IDX).transpose(0, 1)\n",
    "        # False表示not masked，True表示masked\n",
    "        # 用于mask掉Encoder的Token序列中的padding部分，[batch_size, src_len]\n",
    "        tgt_padding_mask = (tgt == self.PAD_IDX).transpose(0, 1)\n",
    "        # 用于mask掉Decoder的Token序列中的padding部分,batch_size, tgt_len\n",
    "        return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
